{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPSy3VPHL5Uq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "targetsize =(200,200)\n",
    "kernel = np.ones((3,3), np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def preprocess_input_img(test_image_path):\n",
    "    '''\n",
    "    This function takes the path to the input test image\n",
    "    and returns a preprocessed image (which can be used as a input to the model)\n",
    "    (Input): Single test image path\n",
    "    (Output): Preprocessed image\n",
    "    '''\n",
    "\n",
    "    # Read the original test image\n",
    "    orig_sample_test_img = cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB)\n",
    "    x = 50\n",
    "    y = 0\n",
    "    width = 200\n",
    "    height = 200\n",
    "\n",
    "# Crop the image\n",
    "    orig_sample_test_img = orig_sample_test_img[y:y+height, x:x+width]\n",
    "    # Convert image to gray scale\n",
    "    gray_sample_test_img = cv2.cvtColor(orig_sample_test_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Resizing image to desired input size\n",
    "    gray_resized_test_img = cv2.resize(gray_sample_test_img, targetsize,\n",
    "                        interpolation = cv2.INTER_AREA)   # To shrink an image\n",
    "\n",
    "    # Remove blemishes from image (if any)\n",
    "    # Remove blemishes from image (if any)\n",
    "    (thresh, black_n_white_sample_img) = cv2.threshold(gray_resized_test_img, 70,255, cv2.THRESH_BINARY_INV)\n",
    "#     black_n_white_sample_img =cv2.GaussianBlur(black_n_white_sample_img , (3, 3), 0)\n",
    "    black_n_white_sample_img= cv2.dilate(black_n_white_sample_img, kernel, iterations=1)\n",
    "\n",
    "    _, black_n_white_sample_img = cv2.threshold(black_n_white_sample_img, 50, 255, cv2.THRESH_BINARY)\n",
    "    black_n_white_sample_img = black_n_white_sample_img/255\n",
    "    # Display Images\t: Plot Sample Input and Preprocessed Test Image\n",
    "\n",
    "#     f = plt.figure(figsize=(10,5))\n",
    "#     ax1 = f.add_subplot(121)\n",
    "#     ax2 = f.add_subplot(122)\n",
    "\n",
    "#     ax1.imshow(np.squeeze(orig_sample_test_img), cmap='gray')\n",
    "#     ax1.set_title(\"Original Test Input Image\", pad=15, fontsize=13, fontweight='bold')\n",
    "#     ax2.imshow(np.squeeze(black_n_white_sample_img), cmap='gray')\n",
    "#     ax2.set_title(\"Preprocessed Test Input Image\", pad=15, fontsize=13, fontweight='bold')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "    return orig_sample_test_img, black_n_white_sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, target_size=targetsize):\n",
    "    _,img = preprocess_input_img(image_path)\n",
    "#     img = img/255\n",
    "    transform = transforms.Compose([\n",
    "#         transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    img = transform(img)\n",
    "    img = img.float()\n",
    "#     print(img.type())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V2SCe0hDNeaV",
    "outputId": "02ea31cc-a3e8-4b9e-da02-1f2821ca4e52"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, main_folder, target_size=targetsize):\n",
    "        self.main_folder = main_folder\n",
    "        self.target_size = target_size\n",
    "        self.data = []\n",
    "\n",
    "        for subdir, _, files in os.walk(main_folder):\n",
    "#             print(\"=\")\n",
    "            if 'image_data.csv' in files:\n",
    "                csv_path = os.path.join(subdir, 'image_data.csv')\n",
    "                df = pd.read_csv(csv_path)\n",
    "                target_image_path = os.path.join(subdir, 'target.jpg')\n",
    "                for _, row in df.iterrows():\n",
    "                    X2_image_path = os.path.join(subdir, row['Image_Name'])\n",
    "                    self.data.append((target_image_path, X2_image_path, row['x'], row['y']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_image_path, X2_image_path, y1, y2 = self.data[idx]\n",
    "        target_image = load_image(target_image_path, self.target_size)\n",
    "        X2_image = load_image(X2_image_path, self.target_size)\n",
    "        return (target_image, X2_image), torch.tensor([y1, y2], dtype=torch.float32)\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "\n",
    "main_folder = './processed'  # Replace with your main folder path\n",
    "dataset = CustomDataset(main_folder)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for child in resnet.children():\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for child in resnet.children():\n",
    "  count+=1\n",
    "  if count < 7:\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ftrs = resnet.fc.in_features\n",
    "num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomResNet(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (c1): Conv2d(2, 3, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (r): ReLU(inplace=True)\n",
       "  (fc11): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc5): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc6): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc7): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (fc8): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (d): Dropout(p=0.2, inplace=False)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.fc = nn.Identity()\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_outputs=2):\n",
    "        super(CustomResNet, self).__init__()\n",
    "\n",
    "        \n",
    "        self.resnet = resnet\n",
    "        self.c1 = nn.Conv2d(1, 3, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_ftrs * 2, 512)\n",
    "        self.r = nn.ReLU(inplace=True)\n",
    "        self.fc11 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.fc6 = nn.Linear(16, 8)\n",
    "        self.fc7 = nn.Linear(8, num_outputs   )\n",
    "        self.fc8 = nn.Linear(8, 1)\n",
    "\n",
    "        self.d = nn.Dropout(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.c1(x1)\n",
    "        x2 = self.c1(x2)\n",
    "\n",
    "        f1 = self.resnet(x1)\n",
    "        f2 = self.resnet(x2)\n",
    "        combined = torch.cat((f1, f2), dim=1)\n",
    "        x = nn.ReLU()(self.fc1(combined))\n",
    "        x = self.d(x)\n",
    "        x = nn.ReLU()(self.fc11(x))\n",
    "        x = self.d(x)\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = self.d(x)\n",
    "        x = nn.ReLU()(self.fc3(x))\n",
    "        x = self.d(x)\n",
    "        x = nn.ReLU()(self.fc4(x))\n",
    "        x = self.d(x)\n",
    "        x = nn.ReLU()(self.fc5(x))\n",
    "        x = self.d(x)\n",
    "        x = nn.ReLU()(self.fc6(x))\n",
    "        y = self.tanh(self.fc7(x))\n",
    "#         z = self.tanh(self.fc8(x))\n",
    "        return y\n",
    "\n",
    "# resnet.fc = nn.Linear(num_ftrs, 2)\n",
    "model = CustomResNet()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x512 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71/1949663948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_71/3876720887.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         f2 = self.resnet(x2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         combined = torch.cat((f1, f2), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x512 and 1024x512)"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, [(1, 200, 200), (1, 200, 200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(epoch, train_loss, val_loss, train_mae, val_mae, train_accuracy, val_accuracy, output_dir):\n",
    "    with open(os.path.join(output_dir, 'training_results.txt'), 'a') as f:\n",
    "        f.write(f'Epoch [{epoch+1}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, '\n",
    "                f'Train MAE: {train_mae:.4f}, Validation MAE: {val_mae:.4f}, '\n",
    "                f'Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n')\n",
    "\n",
    "\n",
    "# Get the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "\n",
    "# Create a directory to save results using the current timestamp\n",
    "output_dir = f'results_pretarin{current_time}'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eY3rPw7wPVEe",
    "outputId": "213b5a1c-d9f4-4659-f6dc-b6ff9d0d1a44"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize lists to store losses, MAE, and accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Define MAE criterion\n",
    "mae_criterion = nn.L1Loss()\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy(outputs, labels):\n",
    "    preds = outputs.round()\n",
    "    corrects = torch.sum(torch.all(preds == labels, dim=1)).item()\n",
    "    return corrects / len(labels)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 50\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mae = 0.0\n",
    "    running_corrects = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        (target_images, X2_images), labels = inputs, labels\n",
    "        target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(target_images, X2_images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        mae = mae_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_mae += mae.item()\n",
    "        running_corrects += accuracy(outputs, labels)\n",
    "#         print(\"=\")\n",
    "        \n",
    "    # Store the training loss, MAE, and accuracy\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_maes.append(running_mae / len(train_loader))\n",
    "    train_accuracies.append(running_corrects / len(train_loader))\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_mae = 0.0\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            (target_images, X2_images), labels = data\n",
    "            target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "            outputs = model(target_images, X2_images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            mae = mae_criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_mae += mae.item()\n",
    "            val_corrects += accuracy(outputs, labels)\n",
    "            if val_corrects > best_accuracy:\n",
    "                best_accuracy= val_corrects\n",
    "                best_model = copy.deepcopy(model)\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_maes.append(val_mae / len(val_loader))\n",
    "    val_accuracies.append(val_corrects / len(val_loader))\n",
    "    \n",
    "    save_results(epoch, train_losses[-1], val_losses[-1], train_maes[-1], val_maes[-1], train_accuracies[-1], val_accuracies[-1], output_dir)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}, Train MAE: {train_maes[-1]:.4f}, Validation MAE: {val_maes[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {val_accuracies[-1]:.4f}')\n",
    "\n",
    "print('Training Done')\n",
    "\n",
    "# Plot the training and validation loss, MAE, and accuracy\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_maes, label='Train MAE')\n",
    "plt.plot(val_maes, label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation MAE')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.savefig(os.path.join(output_dir, 'Training_validation_plots.png'))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeVRGN56hkNZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        (target_images, X2_images), labels = data\n",
    "        target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "#         print(target_images.size())\n",
    "        \n",
    "        outputs = model(target_images, X2_images)\n",
    "#         print(outputs.size())\n",
    "#         print(\"____\\n\")\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(val_loader)\n",
    "print('Validation Loss: ', avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a function to convert continuous values to discrete classes\n",
    "def continuous_to_discrete(y):\n",
    "    y_discrete = np.zeros_like(y)\n",
    "    y_discrete[y > 0.5] = 1\n",
    "    y_discrete[y < -0.5] = -1\n",
    "    return y_discrete\n",
    "\n",
    "# Define a function to extract true and predicted values\n",
    "def extract_true_and_predicted_values(loader, model, device):\n",
    "    model.eval()\n",
    "    true_y1 = []\n",
    "    true_y2 = []\n",
    "    pred_y1 = []\n",
    "    pred_y2 = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            (target_images, X2_images), labels = data\n",
    "            target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "            outputs = model(target_images, X2_images)\n",
    "\n",
    "            true_y1.extend(labels[:, 0].cpu().numpy())\n",
    "            true_y2.extend(labels[:, 1].cpu().numpy())\n",
    "            pred_y1.extend(outputs[:, 0].cpu().numpy())\n",
    "            pred_y2.extend(outputs[:, 1].cpu().numpy())\n",
    "\n",
    "    return np.array(true_y1), np.array(pred_y1), np.array(true_y2), np.array(pred_y2)\n",
    "\n",
    "# Extract true and predicted values\n",
    "true_y1, pred_y1, true_y2, pred_y2 = extract_true_and_predicted_values(val_loader, model, device)\n",
    "\n",
    "# Convert continuous predictions and true values to discrete classes\n",
    "true_y1_discrete = continuous_to_discrete(true_y1)\n",
    "pred_y1_discrete = continuous_to_discrete(pred_y1)\n",
    "true_y2_discrete = continuous_to_discrete(true_y2)\n",
    "pred_y2_discrete = continuous_to_discrete(pred_y2)\n",
    "\n",
    "# print(true_y1_discrete-pred_y1_discrete)\n",
    "# print()\n",
    "# Generate confusion matrices\n",
    "cm_y1 = confusion_matrix(true_y1_discrete, pred_y1_discrete , labels=[-1, 0, 1])\n",
    "cm_y2 = confusion_matrix(true_y2_discrete, pred_y2_discrete , labels=[-1, 0, 1])\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# ConfusionMatrixDisplay(cm_y1).plot(ax=ax[0])\n",
    "sns.heatmap(cm_y1, annot=True, fmt='d', cmap='Blues', ax=ax[0], cbar=False)\n",
    "\n",
    "ax[0].set_title('Confusion Matrix for y1')\n",
    "ax[0].set_xticklabels(['-1', '0', '1'])\n",
    "ax[0].set_yticklabels(['-1', '0', '1'])\n",
    "\n",
    "# ConfusionMatrixDisplay(cm_y2).plot(ax=ax[1])\n",
    "sns.heatmap(cm_y2, annot=True, fmt='d', cmap='Blues', ax=ax[1], cbar=False)\n",
    "\n",
    "ax[1].set_title('Confusion Matrix for y2')\n",
    "ax[1].set_xticklabels(['-1', '0', '1'])\n",
    "ax[1].set_yticklabels(['-1', '0', '1'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = './processed/D9/img027.jpg'\n",
    "target_image_path= './processed/D9/target.jpg'\n",
    "\n",
    "# Load the images\n",
    "target_image = load_image(target_image_path).unsqueeze(0).to(device)  # Adding batch dimension\n",
    "test_image = load_image(test_image_path).unsqueeze(0).to(device)     # Adding batch dimension\n",
    "\n",
    "print(\"target state image is of shape\",target_image.shape)  # Expected: torch.Size([1, 1, 200, 200])\n",
    "print(\"current state frame is of shape\",test_image.shape)    # Expected: torch.Size([1, 1, 200, 200])\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(target_image, test_image)\n",
    "    print(\"model predictions\",outputs)\n",
    "    nparray = outputs.cpu().numpy()  # Move tensor to CPU before conversion\n",
    "#     if nparray[0]<0.5 and nparray[0]>-0.5:\n",
    "#         nparray[0]=0\n",
    "#     if nparray[0]>0.5:\n",
    "#         nparray[0]=1\n",
    "#     if nparray[0]<-0.5:\n",
    "#         nparray[0]=-1\n",
    "    \n",
    "#     print(\"one hot encoding\",nparray.size)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_71/82469876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'results_pretrain{current_time}resnet_last.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'results_pretrain{current_time}best_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# torch.save(f'results_{current_time}resnet_.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_time' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'results_pretrain{current_time}resnet_last.pth')\n",
    "torch.save(best_model, f'results_pretrain{current_time}best_model.pt')\n",
    "# torch.save(f'results_{current_time}resnet_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CIFAR10-ResNet50_85%.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
