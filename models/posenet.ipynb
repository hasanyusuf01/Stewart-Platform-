{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPSy3VPHL5Uq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/posenet\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "targetsize =(224,224)\n",
    "kernel = np.ones((3,3), np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFFakdIfNEEZ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euler_to_quaternion(roll, pitch, yaw):\n",
    "    # Convert degrees to radians\n",
    "    roll = math.radians(roll)\n",
    "    pitch = math.radians(pitch)\n",
    "    yaw = math.radians(yaw)\n",
    "\n",
    "    cy = math.cos(yaw * 0.5)\n",
    "    sy = math.sin(yaw * 0.5)\n",
    "    cp = math.cos(pitch * 0.5)\n",
    "    sp = math.sin(pitch * 0.5)\n",
    "    cr = math.cos(roll * 0.5)\n",
    "    sr = math.sin(roll * 0.5)\n",
    "\n",
    "    w = cr * cp * cy + sr * sp * sy\n",
    "    x = sr * cp * cy - cr * sp * sy\n",
    "    y = cr * sp * cy + sr * cp * sy\n",
    "    z = cr * cp * sy - sr * sp * cy\n",
    "\n",
    "    return w, x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def preprocess_input_img(test_image_path):\n",
    "    '''\n",
    "    This function takes the path to the input test image\n",
    "    and returns a preprocessed image (which can be used as a input to the model)\n",
    "    (Input): Single test image path\n",
    "    (Output): Preprocessed image\n",
    "    '''\n",
    "\n",
    "    # Read the original test image\n",
    "    orig_sample_test_img = cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB)\n",
    "    x = 50\n",
    "    y = 0\n",
    "    width = 200\n",
    "    height = 200\n",
    "\n",
    "# Crop the image\n",
    "    orig_sample_test_img = orig_sample_test_img[y:y+height, x:x+width]\n",
    "    # Convert image to gray scale\n",
    "    gray_sample_test_img = cv2.cvtColor(orig_sample_test_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Resizing image to desired input size\n",
    "    gray_resized_test_img = cv2.resize(gray_sample_test_img, targetsize,\n",
    "                        interpolation = cv2.INTER_AREA)   # To shrink an image\n",
    "\n",
    "    # Remove blemishes from image (if any)\n",
    "    (thresh, black_n_white_sample_img) = cv2.threshold(gray_resized_test_img, 70,255, cv2.THRESH_BINARY_INV)\n",
    "    black_n_white_sample_img= cv2.dilate(black_n_white_sample_img, kernel, iterations=1)\n",
    "\n",
    "    _, black_n_white_sample_img = cv2.threshold(black_n_white_sample_img, 50, 255, cv2.THRESH_BINARY)\n",
    "    black_n_white_sample_img = black_n_white_sample_img/255\n",
    "    return orig_sample_test_img, black_n_white_sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_image(image_path, target_size=targetsize):\n",
    "    try:\n",
    "        _,img = preprocess_input_img(image_path)\n",
    "        transform = transforms.Compose([\n",
    "#         transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        img = transform(img)\n",
    "        img = img.float()\n",
    "#     print(img.type())\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V2SCe0hDNeaV",
    "outputId": "02ea31cc-a3e8-4b9e-da02-1f2821ca4e52"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, main_folder, target_size=targetsize, step_size=10):\n",
    "        self.main_folder = main_folder\n",
    "        self.target_size = target_size\n",
    "        self.step_size = step_size\n",
    "        self.data = []\n",
    "\n",
    "        for subdir, _, files in os.walk(main_folder):\n",
    "#             print(subdir, _)\n",
    "            if 'image_data.csv' in files:\n",
    "                csv_path = os.path.join(subdir, 'image_data.csv')\n",
    "                df = pd.read_csv(csv_path)\n",
    "                target_image_path = os.path.join(subdir, 'target.jpg')\n",
    "                for _, row in df.iterrows():\n",
    "                    X2_image_path = os.path.join(subdir, row['Image_Name'])\n",
    "                    self.data.append((target_image_path, X2_image_path, row['x'], row['y'], row['z'], row['roll'], row['pitch'], row['yaw']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_image_path, X2_image_path, x, y, z, roll, pitch, yaw = self.data[idx]\n",
    "        \n",
    "        # Debugging print statements\n",
    "#         print(f\"Loading target image from: {target_image_path}\")\n",
    "#         print(f\"Loading X2 image from: {X2_image_path}\")\n",
    "        \n",
    "        target_image = load_image(target_image_path, self.target_size)\n",
    "        X2_image = load_image(X2_image_path, self.target_size)\n",
    "\n",
    "        # Check if load_image returns None\n",
    "        if target_image is None or X2_image is None:\n",
    "            print(f\"Failed to load images: {target_image_path}, {X2_image_path}\")\n",
    "            return None\n",
    "\n",
    "        # Convert roll, pitch, yaw to quaternion\n",
    "        roll *= self.step_size\n",
    "        pitch *= self.step_size\n",
    "        yaw *= self.step_size\n",
    "        w, p, q, r = euler_to_quaternion(roll, pitch, yaw)\n",
    "        poses = torch.tensor([x, y], dtype=torch.float32)\n",
    "        return (target_image, X2_image), poses\n",
    "\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "main_folder = '/workspace/processed/'  # Replace with your main folder path\n",
    "dataset = CustomDataset(main_folder)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVrCGU9jNkJb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoseNet(\n",
       "  (pre_layers): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(8, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(16, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(16, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (7): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
       "    (10): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(128, 192, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
       "    (15): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lastlayers): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (8): Dropout(p=0.4, inplace=False)\n",
       "    (9): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (10): Dropout(p=0.4, inplace=False)\n",
       "    (11): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (12): Dropout(p=0.4, inplace=False)\n",
       "    (13): Linear(in_features=8, out_features=2, bias=True)\n",
       "    (14): Tanh()\n",
       "  )\n",
       "  (a3): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b3): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (a4): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(960, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(960, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(960, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b4): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (c4): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (d4): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (e4): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (a5): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (b5): InceptionV1(\n",
       "    (b1): Sequential(\n",
       "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (avg_pool5x5): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "  (conv1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1x12): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (fc2048): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (dropout5): Dropout(p=0.5, inplace=False)\n",
       "  (dropout7): Dropout(p=0.7, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (cls_fc_pose_xyz): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (cls_fc_pose_xyz_1024): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['PoseNet', 'posenet_v1', 'PoseLoss']\n",
    "\n",
    "class InceptionV1(nn.Module):\n",
    "    def __init__(self, in_channels, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(InceptionV1, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n1x1, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n3x3red, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n5x5red, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_planes, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1, y2, y3, y4], 1)\n",
    "\n",
    "\n",
    "# PoseNet\n",
    "class PoseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseNet, self).__init__()\n",
    "\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv2d(8, 8, kernel_size=2, stride=1, padding=1),\n",
    "                        nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(8, 16, kernel_size=2, stride=1, padding=1),\n",
    "            nn.Conv2d(16, 16, kernel_size=2, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.Conv2d(64, 64, kernel_size=2, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.LocalResponseNorm(5, 0.0001, 0.75),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 192, kernel_size=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.LocalResponseNorm(5, 0.0001, 0.75),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.lastlayers= nn.Sequential(  nn.Dropout(p=0.5),     \n",
    "            nn.Linear(512,256),\n",
    "#             nn.ReLU(True),\n",
    "                                       nn.Dropout(p=0.4),\n",
    "         nn.Linear(256, 128),\n",
    "#             nn.ReLU(True),\n",
    "                                       nn.Dropout(p=0.4),\n",
    "         nn.Linear(128, 64),\n",
    "#             nn.ReLU(True),\n",
    "                                       nn.Dropout(p=0.4),\n",
    "         nn.Linear(64, 32),\n",
    "#             nn.ReLU(True),\n",
    "                                       nn.Dropout(p=0.4),\n",
    "        nn.Linear(32, 16),\n",
    "#             nn.ReLU(True),\n",
    "                                       nn.Dropout(p=0.4),\n",
    "        nn.Linear(16, 8),\n",
    "#             nn.ReLU(True),\n",
    "                                       nn.Dropout(p=0.4),\n",
    "        nn.Linear(8, 2),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.a3 = InceptionV1(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = InceptionV1(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.a4 = InceptionV1(960, 192, 96, 208, 16, 48, 64)\n",
    "        self.b4 = InceptionV1(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.c4 = InceptionV1(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.d4 = InceptionV1(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.e4 = InceptionV1(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = InceptionV1(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = InceptionV1(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.avg_pool5x5 = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv1x1 = nn.Conv2d(512, 128, kernel_size=1, stride=1)\n",
    "        self.conv1x12 = nn.Conv2d(528, 128, kernel_size=1, stride=1)\n",
    "        self.fc = nn.Linear(1024, 2048)\n",
    "        self.fc2048 = nn.Linear(2048, 1024)\n",
    "\n",
    "        self.dropout5 = nn.Dropout(p=0.5)\n",
    "        self.dropout7 = nn.Dropout(p=0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.cls_fc_pose_xyz = nn.Linear(2048, 1024)\n",
    "#         self.cls_fc_pose_wpqr = nn.Linear(2048, 4)\n",
    "        self.cls_fc_pose_xyz_1024 = nn.Linear(1024, 512)\n",
    "#         self.cls_fc_pose_wpqr_1024 = nn.Linear(1024, 4)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "        out = self.pre_layers(x1)\n",
    "        out = self.a3(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.max_pool(out)\n",
    "#         out = self.a4(out)\n",
    "        \n",
    "        out2 = self.pre_layers(x1)\n",
    "        out2 = self.a3(out2)\n",
    "        out2 = self.b3(out2)\n",
    "        out2 = self.max_pool(out2)\n",
    "            \n",
    "        \n",
    "        out = torch.cat((out, out2), dim=1)\n",
    "        out = self.a4(out)   \n",
    "#         out = self.max_pool(out)\n",
    "\n",
    "        cls1_pool = self.avg_pool5x5(out)\n",
    "        cls1_reduction = self.conv1x1(cls1_pool)\n",
    "        cls1_reduction = F.relu(cls1_reduction)\n",
    "        cls1_reduction = cls1_reduction.view(cls1_reduction.size(0), -1)\n",
    "        cls1_fc1 = self.fc2048(cls1_reduction)\n",
    "        cls1_fc1 = self.relu(cls1_fc1)\n",
    "        cls1_fc1 = self.dropout7(cls1_fc1)\n",
    "        cls1_fc_pose_xyz = self.cls_fc_pose_xyz_1024(cls1_fc1)\n",
    "        cls1_fc_pose_xyz = self.lastlayers(cls1_fc_pose_xyz)\n",
    "#         cls1_pose_wpqr = self.cls_fc_pose_wpqr_1024(cls1_fc1)\n",
    "\n",
    "        out = self.b4(out)\n",
    "        out = self.c4(out)\n",
    "        out = self.d4(out)\n",
    "        cls2_pool = self.avg_pool5x5(out)\n",
    "        cls2_reduction = self.conv1x12(cls2_pool)\n",
    "        cls2_reduction = F.relu(cls2_reduction)\n",
    "        cls2_reduction = cls2_reduction.view(cls2_reduction.size(0), -1)\n",
    "        cls2_fc1 = self.fc2048(cls2_reduction)\n",
    "        cls2_fc1 = self.relu(cls2_fc1)\n",
    "        cls2_fc1 = self.dropout7(cls2_fc1)\n",
    "        cls2_fc_pose_xyz = self.cls_fc_pose_xyz_1024(cls2_fc1)\n",
    "        cls2_fc_pose_xyz = self.lastlayers(cls2_fc_pose_xyz)\n",
    "\n",
    "#         cls2_pose_wpqr = self.cls_fc_pose_wpqr_1024(cls2_fc1)\n",
    "        out = self.e4(out)\n",
    "\n",
    "        out = self.max_pool(out)\n",
    "\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        cls3_pool = self.avg_pool(out)\n",
    "        cls3_pool = cls3_pool.view(cls3_pool.size(0), -1)\n",
    "        cls3_fc1 = self.fc(cls3_pool)\n",
    "        cls3_fc1 = self.relu(cls3_fc1)\n",
    "        cls3_fc1 = self.dropout5(cls3_fc1)\n",
    "        cls3_fc_pose_xyz = self.cls_fc_pose_xyz(cls3_fc1)\n",
    "        cls3_fc_pose_xyz = self.cls_fc_pose_xyz_1024(cls3_fc_pose_xyz)\n",
    "        cls3_fc_pose_xyz = self.lastlayers(cls3_fc_pose_xyz)\n",
    "\n",
    "#         cls3_pose_wpqr = self.cls_fc_pose_wpqr(cls3_fc1)\n",
    "\n",
    "        return cls1_fc_pose_xyz, \\\n",
    "               cls2_fc_pose_xyz, \\\n",
    "               cls3_fc_pose_xyz\n",
    "# , \\              cls3_pose_wpqr\n",
    "\n",
    "\n",
    "class PoseLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, w1_x, w2_x, w3_x):\n",
    "        super(PoseLoss, self).__init__()\n",
    "        self.w1_x = w1_x\n",
    "        self.w2_x = w2_x\n",
    "        self.w3_x = w3_x\n",
    "#         self.w1_q = w1_q\n",
    "#         self.w2_q = w2_q\n",
    "#         self.w3_q = w3_q\n",
    "        return\n",
    "\n",
    "    def forward(self, p1_x, p2_x, p3_x, poseGT):\n",
    "        pose_x = poseGT\n",
    "#         pose_q = poseGT[:, 3:]\n",
    "\n",
    "        l1_x = torch.sqrt(torch.sum(Variable(torch.Tensor(np.square(F.pairwise_distance(pose_x, p1_x).detach().cpu().numpy())), requires_grad=True))) * self.w1_x\n",
    "#         l1_q = torch.sqrt(torch.sum(Variable(torch.Tensor(np.square(F.pairwise_distance(pose_q, p1_q).detach().cpu().numpy())), requires_grad=True))) * self.w1_q\n",
    "        l2_x = torch.sqrt(torch.sum(Variable(torch.Tensor(np.square(F.pairwise_distance(pose_x, p2_x).detach().cpu().numpy())), requires_grad=True))) * self.w2_x\n",
    "#         l2_q = torch.sqrt(torch.sum(Variable(torch.Tensor(np.square(F.pairwise_distance(pose_q, p2_q).detach().cpu().numpy())), requires_grad=True))) * self.w2_q\n",
    "        l3_x = torch.sqrt(torch.sum(Variable(torch.Tensor(np.square(F.pairwise_distance(pose_x, p3_x).detach().cpu().numpy())), requires_grad=True))) * self.w3_x\n",
    "#         l3_q = torch.sqrt(torch.sum(Variable(torch.Tensor(np.square(F.pairwise_distance(pose_q, p3_q).detach().cpu().numpy())), requires_grad=True))) * self.w3_q\n",
    "\n",
    "        loss = l1_x  + l2_x + l3_x \n",
    "        return loss\n",
    "\n",
    "\n",
    "# def posenet_v1():\n",
    "#     model = PoseNet()\n",
    "#     return model\n",
    "\n",
    "model = PoseNet()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 225, 225]              40\n",
      "            Conv2d-2          [-1, 8, 226, 226]             264\n",
      "              ReLU-3          [-1, 8, 226, 226]               0\n",
      "            Conv2d-4         [-1, 16, 227, 227]             528\n",
      "            Conv2d-5         [-1, 16, 228, 228]           1,040\n",
      "              ReLU-6         [-1, 16, 228, 228]               0\n",
      "            Conv2d-7         [-1, 64, 114, 114]          50,240\n",
      "            Conv2d-8         [-1, 64, 115, 115]          16,448\n",
      "         MaxPool2d-9           [-1, 64, 57, 57]               0\n",
      "LocalResponseNorm-10           [-1, 64, 57, 57]               0\n",
      "           Conv2d-11          [-1, 128, 56, 56]          32,896\n",
      "             ReLU-12          [-1, 128, 56, 56]               0\n",
      "           Conv2d-13          [-1, 192, 57, 57]          98,496\n",
      "             ReLU-14          [-1, 192, 57, 57]               0\n",
      "LocalResponseNorm-15          [-1, 192, 57, 57]               0\n",
      "        MaxPool2d-16          [-1, 192, 28, 28]               0\n",
      "           Conv2d-17           [-1, 64, 28, 28]          12,352\n",
      "             ReLU-18           [-1, 64, 28, 28]               0\n",
      "           Conv2d-19           [-1, 96, 28, 28]          18,528\n",
      "             ReLU-20           [-1, 96, 28, 28]               0\n",
      "           Conv2d-21          [-1, 128, 28, 28]         110,720\n",
      "             ReLU-22          [-1, 128, 28, 28]               0\n",
      "           Conv2d-23           [-1, 16, 28, 28]           3,088\n",
      "             ReLU-24           [-1, 16, 28, 28]               0\n",
      "           Conv2d-25           [-1, 32, 28, 28]          12,832\n",
      "             ReLU-26           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-27          [-1, 192, 28, 28]               0\n",
      "           Conv2d-28           [-1, 32, 28, 28]           6,176\n",
      "             ReLU-29           [-1, 32, 28, 28]               0\n",
      "      InceptionV1-30          [-1, 256, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]          32,896\n",
      "             ReLU-32          [-1, 128, 28, 28]               0\n",
      "           Conv2d-33          [-1, 128, 28, 28]          32,896\n",
      "             ReLU-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 192, 28, 28]         221,376\n",
      "             ReLU-36          [-1, 192, 28, 28]               0\n",
      "           Conv2d-37           [-1, 32, 28, 28]           8,224\n",
      "             ReLU-38           [-1, 32, 28, 28]               0\n",
      "           Conv2d-39           [-1, 96, 28, 28]          76,896\n",
      "             ReLU-40           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-41          [-1, 256, 28, 28]               0\n",
      "           Conv2d-42           [-1, 64, 28, 28]          16,448\n",
      "             ReLU-43           [-1, 64, 28, 28]               0\n",
      "      InceptionV1-44          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-45          [-1, 480, 14, 14]               0\n",
      "           Conv2d-46          [-1, 8, 225, 225]              40\n",
      "           Conv2d-47          [-1, 8, 226, 226]             264\n",
      "             ReLU-48          [-1, 8, 226, 226]               0\n",
      "           Conv2d-49         [-1, 16, 227, 227]             528\n",
      "           Conv2d-50         [-1, 16, 228, 228]           1,040\n",
      "             ReLU-51         [-1, 16, 228, 228]               0\n",
      "           Conv2d-52         [-1, 64, 114, 114]          50,240\n",
      "           Conv2d-53         [-1, 64, 115, 115]          16,448\n",
      "        MaxPool2d-54           [-1, 64, 57, 57]               0\n",
      "LocalResponseNorm-55           [-1, 64, 57, 57]               0\n",
      "           Conv2d-56          [-1, 128, 56, 56]          32,896\n",
      "             ReLU-57          [-1, 128, 56, 56]               0\n",
      "           Conv2d-58          [-1, 192, 57, 57]          98,496\n",
      "             ReLU-59          [-1, 192, 57, 57]               0\n",
      "LocalResponseNorm-60          [-1, 192, 57, 57]               0\n",
      "        MaxPool2d-61          [-1, 192, 28, 28]               0\n",
      "           Conv2d-62           [-1, 64, 28, 28]          12,352\n",
      "             ReLU-63           [-1, 64, 28, 28]               0\n",
      "           Conv2d-64           [-1, 96, 28, 28]          18,528\n",
      "             ReLU-65           [-1, 96, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         110,720\n",
      "             ReLU-67          [-1, 128, 28, 28]               0\n",
      "           Conv2d-68           [-1, 16, 28, 28]           3,088\n",
      "             ReLU-69           [-1, 16, 28, 28]               0\n",
      "           Conv2d-70           [-1, 32, 28, 28]          12,832\n",
      "             ReLU-71           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-72          [-1, 192, 28, 28]               0\n",
      "           Conv2d-73           [-1, 32, 28, 28]           6,176\n",
      "             ReLU-74           [-1, 32, 28, 28]               0\n",
      "      InceptionV1-75          [-1, 256, 28, 28]               0\n",
      "           Conv2d-76          [-1, 128, 28, 28]          32,896\n",
      "             ReLU-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78          [-1, 128, 28, 28]          32,896\n",
      "             ReLU-79          [-1, 128, 28, 28]               0\n",
      "           Conv2d-80          [-1, 192, 28, 28]         221,376\n",
      "             ReLU-81          [-1, 192, 28, 28]               0\n",
      "           Conv2d-82           [-1, 32, 28, 28]           8,224\n",
      "             ReLU-83           [-1, 32, 28, 28]               0\n",
      "           Conv2d-84           [-1, 96, 28, 28]          76,896\n",
      "             ReLU-85           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-86          [-1, 256, 28, 28]               0\n",
      "           Conv2d-87           [-1, 64, 28, 28]          16,448\n",
      "             ReLU-88           [-1, 64, 28, 28]               0\n",
      "      InceptionV1-89          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-90          [-1, 480, 14, 14]               0\n",
      "           Conv2d-91          [-1, 192, 14, 14]         184,512\n",
      "             ReLU-92          [-1, 192, 14, 14]               0\n",
      "           Conv2d-93           [-1, 96, 14, 14]          92,256\n",
      "             ReLU-94           [-1, 96, 14, 14]               0\n",
      "           Conv2d-95          [-1, 208, 14, 14]         179,920\n",
      "             ReLU-96          [-1, 208, 14, 14]               0\n",
      "           Conv2d-97           [-1, 16, 14, 14]          15,376\n",
      "             ReLU-98           [-1, 16, 14, 14]               0\n",
      "           Conv2d-99           [-1, 48, 14, 14]          19,248\n",
      "            ReLU-100           [-1, 48, 14, 14]               0\n",
      "       MaxPool2d-101          [-1, 960, 14, 14]               0\n",
      "          Conv2d-102           [-1, 64, 14, 14]          61,504\n",
      "            ReLU-103           [-1, 64, 14, 14]               0\n",
      "     InceptionV1-104          [-1, 512, 14, 14]               0\n",
      "       AvgPool2d-105            [-1, 512, 4, 4]               0\n",
      "          Conv2d-106            [-1, 128, 4, 4]          65,664\n",
      "          Linear-107                 [-1, 1024]       2,098,176\n",
      "            ReLU-108                 [-1, 1024]               0\n",
      "         Dropout-109                 [-1, 1024]               0\n",
      "          Linear-110                  [-1, 512]         524,800\n",
      "         Dropout-111                  [-1, 512]               0\n",
      "          Linear-112                  [-1, 256]         131,328\n",
      "         Dropout-113                  [-1, 256]               0\n",
      "          Linear-114                  [-1, 128]          32,896\n",
      "         Dropout-115                  [-1, 128]               0\n",
      "          Linear-116                   [-1, 64]           8,256\n",
      "         Dropout-117                   [-1, 64]               0\n",
      "          Linear-118                   [-1, 32]           2,080\n",
      "         Dropout-119                   [-1, 32]               0\n",
      "          Linear-120                   [-1, 16]             528\n",
      "         Dropout-121                   [-1, 16]               0\n",
      "          Linear-122                    [-1, 8]             136\n",
      "         Dropout-123                    [-1, 8]               0\n",
      "          Linear-124                    [-1, 2]              18\n",
      "            Tanh-125                    [-1, 2]               0\n",
      "          Conv2d-126          [-1, 160, 14, 14]          82,080\n",
      "            ReLU-127          [-1, 160, 14, 14]               0\n",
      "          Conv2d-128          [-1, 112, 14, 14]          57,456\n",
      "            ReLU-129          [-1, 112, 14, 14]               0\n",
      "          Conv2d-130          [-1, 224, 14, 14]         226,016\n",
      "            ReLU-131          [-1, 224, 14, 14]               0\n",
      "          Conv2d-132           [-1, 24, 14, 14]          12,312\n",
      "            ReLU-133           [-1, 24, 14, 14]               0\n",
      "          Conv2d-134           [-1, 64, 14, 14]          38,464\n",
      "            ReLU-135           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-136          [-1, 512, 14, 14]               0\n",
      "          Conv2d-137           [-1, 64, 14, 14]          32,832\n",
      "            ReLU-138           [-1, 64, 14, 14]               0\n",
      "     InceptionV1-139          [-1, 512, 14, 14]               0\n",
      "          Conv2d-140          [-1, 128, 14, 14]          65,664\n",
      "            ReLU-141          [-1, 128, 14, 14]               0\n",
      "          Conv2d-142          [-1, 128, 14, 14]          65,664\n",
      "            ReLU-143          [-1, 128, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         295,168\n",
      "            ReLU-145          [-1, 256, 14, 14]               0\n",
      "          Conv2d-146           [-1, 24, 14, 14]          12,312\n",
      "            ReLU-147           [-1, 24, 14, 14]               0\n",
      "          Conv2d-148           [-1, 64, 14, 14]          38,464\n",
      "            ReLU-149           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-150          [-1, 512, 14, 14]               0\n",
      "          Conv2d-151           [-1, 64, 14, 14]          32,832\n",
      "            ReLU-152           [-1, 64, 14, 14]               0\n",
      "     InceptionV1-153          [-1, 512, 14, 14]               0\n",
      "          Conv2d-154          [-1, 112, 14, 14]          57,456\n",
      "            ReLU-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 144, 14, 14]          73,872\n",
      "            ReLU-157          [-1, 144, 14, 14]               0\n",
      "          Conv2d-158          [-1, 288, 14, 14]         373,536\n",
      "            ReLU-159          [-1, 288, 14, 14]               0\n",
      "          Conv2d-160           [-1, 32, 14, 14]          16,416\n",
      "            ReLU-161           [-1, 32, 14, 14]               0\n",
      "          Conv2d-162           [-1, 64, 14, 14]          51,264\n",
      "            ReLU-163           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-164          [-1, 512, 14, 14]               0\n",
      "          Conv2d-165           [-1, 64, 14, 14]          32,832\n",
      "            ReLU-166           [-1, 64, 14, 14]               0\n",
      "     InceptionV1-167          [-1, 528, 14, 14]               0\n",
      "       AvgPool2d-168            [-1, 528, 4, 4]               0\n",
      "          Conv2d-169            [-1, 128, 4, 4]          67,712\n",
      "          Linear-170                 [-1, 1024]       2,098,176\n",
      "            ReLU-171                 [-1, 1024]               0\n",
      "         Dropout-172                 [-1, 1024]               0\n",
      "          Linear-173                  [-1, 512]         524,800\n",
      "         Dropout-174                  [-1, 512]               0\n",
      "          Linear-175                  [-1, 256]         131,328\n",
      "         Dropout-176                  [-1, 256]               0\n",
      "          Linear-177                  [-1, 128]          32,896\n",
      "         Dropout-178                  [-1, 128]               0\n",
      "          Linear-179                   [-1, 64]           8,256\n",
      "         Dropout-180                   [-1, 64]               0\n",
      "          Linear-181                   [-1, 32]           2,080\n",
      "         Dropout-182                   [-1, 32]               0\n",
      "          Linear-183                   [-1, 16]             528\n",
      "         Dropout-184                   [-1, 16]               0\n",
      "          Linear-185                    [-1, 8]             136\n",
      "         Dropout-186                    [-1, 8]               0\n",
      "          Linear-187                    [-1, 2]              18\n",
      "            Tanh-188                    [-1, 2]               0\n",
      "          Conv2d-189          [-1, 256, 14, 14]         135,424\n",
      "            ReLU-190          [-1, 256, 14, 14]               0\n",
      "          Conv2d-191          [-1, 160, 14, 14]          84,640\n",
      "            ReLU-192          [-1, 160, 14, 14]               0\n",
      "          Conv2d-193          [-1, 320, 14, 14]         461,120\n",
      "            ReLU-194          [-1, 320, 14, 14]               0\n",
      "          Conv2d-195           [-1, 32, 14, 14]          16,928\n",
      "            ReLU-196           [-1, 32, 14, 14]               0\n",
      "          Conv2d-197          [-1, 128, 14, 14]         102,528\n",
      "            ReLU-198          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-199          [-1, 528, 14, 14]               0\n",
      "          Conv2d-200          [-1, 128, 14, 14]          67,712\n",
      "            ReLU-201          [-1, 128, 14, 14]               0\n",
      "     InceptionV1-202          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-203            [-1, 832, 7, 7]               0\n",
      "          Conv2d-204            [-1, 256, 7, 7]         213,248\n",
      "            ReLU-205            [-1, 256, 7, 7]               0\n",
      "          Conv2d-206            [-1, 160, 7, 7]         133,280\n",
      "            ReLU-207            [-1, 160, 7, 7]               0\n",
      "          Conv2d-208            [-1, 320, 7, 7]         461,120\n",
      "            ReLU-209            [-1, 320, 7, 7]               0\n",
      "          Conv2d-210             [-1, 32, 7, 7]          26,656\n",
      "            ReLU-211             [-1, 32, 7, 7]               0\n",
      "          Conv2d-212            [-1, 128, 7, 7]         102,528\n",
      "            ReLU-213            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-214            [-1, 832, 7, 7]               0\n",
      "          Conv2d-215            [-1, 128, 7, 7]         106,624\n",
      "            ReLU-216            [-1, 128, 7, 7]               0\n",
      "     InceptionV1-217            [-1, 832, 7, 7]               0\n",
      "          Conv2d-218            [-1, 384, 7, 7]         319,872\n",
      "            ReLU-219            [-1, 384, 7, 7]               0\n",
      "          Conv2d-220            [-1, 192, 7, 7]         159,936\n",
      "            ReLU-221            [-1, 192, 7, 7]               0\n",
      "          Conv2d-222            [-1, 384, 7, 7]         663,936\n",
      "            ReLU-223            [-1, 384, 7, 7]               0\n",
      "          Conv2d-224             [-1, 48, 7, 7]          39,984\n",
      "            ReLU-225             [-1, 48, 7, 7]               0\n",
      "          Conv2d-226            [-1, 128, 7, 7]         153,728\n",
      "            ReLU-227            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-228            [-1, 832, 7, 7]               0\n",
      "          Conv2d-229            [-1, 128, 7, 7]         106,624\n",
      "            ReLU-230            [-1, 128, 7, 7]               0\n",
      "     InceptionV1-231           [-1, 1024, 7, 7]               0\n",
      "       AvgPool2d-232           [-1, 1024, 1, 1]               0\n",
      "          Linear-233                 [-1, 2048]       2,099,200\n",
      "            ReLU-234                 [-1, 2048]               0\n",
      "         Dropout-235                 [-1, 2048]               0\n",
      "          Linear-236                 [-1, 1024]       2,098,176\n",
      "          Linear-237                  [-1, 512]         524,800\n",
      "         Dropout-238                  [-1, 512]               0\n",
      "          Linear-239                  [-1, 256]         131,328\n",
      "         Dropout-240                  [-1, 256]               0\n",
      "          Linear-241                  [-1, 128]          32,896\n",
      "         Dropout-242                  [-1, 128]               0\n",
      "          Linear-243                   [-1, 64]           8,256\n",
      "         Dropout-244                   [-1, 64]               0\n",
      "          Linear-245                   [-1, 32]           2,080\n",
      "         Dropout-246                   [-1, 32]               0\n",
      "          Linear-247                   [-1, 16]             528\n",
      "         Dropout-248                   [-1, 16]               0\n",
      "          Linear-249                    [-1, 8]             136\n",
      "         Dropout-250                    [-1, 8]               0\n",
      "          Linear-251                    [-1, 2]              18\n",
      "            Tanh-252                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 17,605,342\n",
      "Trainable params: 17,605,342\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9604.00\n",
      "Forward/backward pass size (MB): 194.85\n",
      "Params size (MB): 67.16\n",
      "Estimated Total Size (MB): 9866.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, [(1, 224, 224),(1, 224, 224)])\n",
    "# summary(model, (1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(epoch, train_loss, val_loss, train_mae, val_mae, train_accuracy, val_accuracy, output_dir):\n",
    "    with open(os.path.join(output_dir, 'training_results.txt'), 'a') as f:\n",
    "        f.write(f'Epoch [{epoch+1}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, '\n",
    "                f'Train MAE: {train_mae:.4f}, Validation MAE: {val_mae:.4f}, '\n",
    "                f'Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n')\n",
    "\n",
    "\n",
    "Get the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "\n",
    "# Create a directory to save results using the current timestamp\n",
    "output_dir = f'results_{current_time}'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "EPOCHS = 80000\n",
    "\n",
    "criterion = PoseLoss(0.3, 0.3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80000], Train Loss: 7.7732, Validation Loss: 7.6935\n",
      "Epoch [2/80000], Train Loss: 7.7810, Validation Loss: 7.6935\n",
      "Epoch [3/80000], Train Loss: 7.7564, Validation Loss: 7.6935\n",
      "Epoch [4/80000], Train Loss: 7.7718, Validation Loss: 7.6935\n",
      "Epoch [5/80000], Train Loss: 7.7782, Validation Loss: 7.6935\n",
      "Epoch [6/80000], Train Loss: 7.7723, Validation Loss: 7.6935\n",
      "Epoch [7/80000], Train Loss: 7.7609, Validation Loss: 7.6935\n",
      "Epoch [8/80000], Train Loss: 7.7804, Validation Loss: 7.6935\n",
      "Epoch [9/80000], Train Loss: 7.7483, Validation Loss: 7.6935\n",
      "Epoch [10/80000], Train Loss: 7.7877, Validation Loss: 7.6935\n",
      "Epoch [11/80000], Train Loss: 7.7739, Validation Loss: 7.6935\n",
      "Epoch [12/80000], Train Loss: 7.7837, Validation Loss: 7.6935\n",
      "Epoch [13/80000], Train Loss: 7.7715, Validation Loss: 7.6935\n",
      "Epoch [14/80000], Train Loss: 7.7679, Validation Loss: 7.6935\n",
      "Epoch [15/80000], Train Loss: 7.7631, Validation Loss: 7.6935\n",
      "Epoch [16/80000], Train Loss: 7.7810, Validation Loss: 7.6935\n",
      "Epoch [17/80000], Train Loss: 7.7761, Validation Loss: 7.6935\n",
      "Epoch [18/80000], Train Loss: 7.7858, Validation Loss: 7.6935\n",
      "Epoch [19/80000], Train Loss: 7.7767, Validation Loss: 7.6935\n",
      "Epoch [20/80000], Train Loss: 7.7694, Validation Loss: 7.6935\n",
      "Epoch [21/80000], Train Loss: 7.8020, Validation Loss: 7.6935\n",
      "Epoch [22/80000], Train Loss: 7.7725, Validation Loss: 7.6935\n",
      "Epoch [23/80000], Train Loss: 7.7654, Validation Loss: 7.6935\n",
      "Epoch [24/80000], Train Loss: 7.7842, Validation Loss: 7.6935\n",
      "Epoch [25/80000], Train Loss: 7.7820, Validation Loss: 7.6935\n",
      "Epoch [26/80000], Train Loss: 7.7817, Validation Loss: 7.6935\n",
      "Epoch [27/80000], Train Loss: 7.7797, Validation Loss: 7.6935\n",
      "Epoch [28/80000], Train Loss: 7.7674, Validation Loss: 7.6935\n",
      "Epoch [29/80000], Train Loss: 7.7757, Validation Loss: 7.6935\n",
      "Epoch [30/80000], Train Loss: 7.7616, Validation Loss: 7.6935\n",
      "Epoch [31/80000], Train Loss: 7.7694, Validation Loss: 7.6935\n",
      "Epoch [32/80000], Train Loss: 7.7577, Validation Loss: 7.6935\n",
      "Epoch [33/80000], Train Loss: 7.7579, Validation Loss: 7.6935\n",
      "Epoch [34/80000], Train Loss: 7.7947, Validation Loss: 7.6935\n",
      "Epoch [35/80000], Train Loss: 7.7796, Validation Loss: 7.6935\n",
      "Epoch [36/80000], Train Loss: 7.7783, Validation Loss: 7.6935\n",
      "Epoch [37/80000], Train Loss: 7.7850, Validation Loss: 7.6935\n",
      "Epoch [38/80000], Train Loss: 7.7786, Validation Loss: 7.6935\n",
      "Epoch [39/80000], Train Loss: 7.7806, Validation Loss: 7.6935\n",
      "Epoch [40/80000], Train Loss: 7.7673, Validation Loss: 7.6935\n",
      "Epoch [41/80000], Train Loss: 7.7687, Validation Loss: 7.6935\n",
      "Epoch [42/80000], Train Loss: 7.7636, Validation Loss: 7.6935\n",
      "Epoch [43/80000], Train Loss: 7.7778, Validation Loss: 7.6935\n",
      "Epoch [44/80000], Train Loss: 7.7718, Validation Loss: 7.6935\n",
      "Epoch [45/80000], Train Loss: 7.7959, Validation Loss: 7.6935\n",
      "Epoch [46/80000], Train Loss: 7.7596, Validation Loss: 7.6935\n",
      "Epoch [47/80000], Train Loss: 7.7854, Validation Loss: 7.6935\n",
      "Epoch [48/80000], Train Loss: 7.7842, Validation Loss: 7.6935\n",
      "Epoch [49/80000], Train Loss: 7.7720, Validation Loss: 7.6935\n",
      "Epoch [50/80000], Train Loss: 7.7919, Validation Loss: 7.6935\n",
      "Epoch [51/80000], Train Loss: 7.7767, Validation Loss: 7.6935\n",
      "Epoch [52/80000], Train Loss: 7.7715, Validation Loss: 7.6935\n",
      "Epoch [53/80000], Train Loss: 7.7832, Validation Loss: 7.6935\n",
      "Epoch [54/80000], Train Loss: 7.7837, Validation Loss: 7.6935\n",
      "Epoch [55/80000], Train Loss: 7.7906, Validation Loss: 7.6935\n",
      "Epoch [56/80000], Train Loss: 7.7693, Validation Loss: 7.6935\n",
      "Epoch [57/80000], Train Loss: 7.7743, Validation Loss: 7.6935\n",
      "Epoch [58/80000], Train Loss: 7.8021, Validation Loss: 7.6935\n",
      "Epoch [59/80000], Train Loss: 7.7708, Validation Loss: 7.6935\n",
      "Epoch [60/80000], Train Loss: 7.7816, Validation Loss: 7.6935\n",
      "Epoch [61/80000], Train Loss: 7.7920, Validation Loss: 7.6935\n",
      "Epoch [62/80000], Train Loss: 7.7882, Validation Loss: 7.6935\n",
      "Epoch [63/80000], Train Loss: 7.7825, Validation Loss: 7.6935\n",
      "Epoch [64/80000], Train Loss: 7.7836, Validation Loss: 7.6935\n",
      "Epoch [65/80000], Train Loss: 7.7751, Validation Loss: 7.6935\n",
      "Epoch [66/80000], Train Loss: 7.7757, Validation Loss: 7.6935\n",
      "Epoch [67/80000], Train Loss: 7.7891, Validation Loss: 7.6935\n",
      "Epoch [68/80000], Train Loss: 7.7907, Validation Loss: 7.6935\n",
      "Epoch [69/80000], Train Loss: 7.7871, Validation Loss: 7.6935\n",
      "Epoch [70/80000], Train Loss: 7.7773, Validation Loss: 7.6935\n",
      "Epoch [71/80000], Train Loss: 7.7774, Validation Loss: 7.6935\n",
      "Epoch [72/80000], Train Loss: 7.7728, Validation Loss: 7.6935\n",
      "Epoch [73/80000], Train Loss: 7.7765, Validation Loss: 7.6935\n",
      "Epoch [74/80000], Train Loss: 7.7596, Validation Loss: 7.6935\n",
      "Epoch [75/80000], Train Loss: 7.7742, Validation Loss: 7.6935\n",
      "Epoch [76/80000], Train Loss: 7.7867, Validation Loss: 7.6935\n",
      "Epoch [77/80000], Train Loss: 7.7905, Validation Loss: 7.6935\n",
      "Epoch [78/80000], Train Loss: 7.7688, Validation Loss: 7.6935\n",
      "Epoch [79/80000], Train Loss: 7.7624, Validation Loss: 7.6935\n",
      "Epoch [80/80000], Train Loss: 7.7890, Validation Loss: 7.6935\n",
      "Epoch [81/80000], Train Loss: 7.7758, Validation Loss: 7.6935\n",
      "Epoch [82/80000], Train Loss: 7.8071, Validation Loss: 7.6935\n",
      "Epoch [83/80000], Train Loss: 7.7556, Validation Loss: 7.6935\n",
      "Epoch [84/80000], Train Loss: 7.7702, Validation Loss: 7.6935\n",
      "Epoch [85/80000], Train Loss: 7.7780, Validation Loss: 7.6935\n",
      "Epoch [86/80000], Train Loss: 7.7938, Validation Loss: 7.6935\n",
      "Epoch [87/80000], Train Loss: 7.7726, Validation Loss: 7.6935\n",
      "Epoch [88/80000], Train Loss: 7.7630, Validation Loss: 7.6935\n",
      "Epoch [89/80000], Train Loss: 7.7718, Validation Loss: 7.6935\n",
      "Epoch [90/80000], Train Loss: 7.7643, Validation Loss: 7.6935\n",
      "Epoch [91/80000], Train Loss: 7.7815, Validation Loss: 7.6935\n",
      "Epoch [92/80000], Train Loss: 7.7740, Validation Loss: 7.6935\n",
      "Epoch [93/80000], Train Loss: 7.7842, Validation Loss: 7.6935\n",
      "Epoch [94/80000], Train Loss: 7.7779, Validation Loss: 7.6935\n",
      "Epoch [95/80000], Train Loss: 7.7968, Validation Loss: 7.6935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1649/3306529463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#    running_mae = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#    running_corrects = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtarget_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1649/1395285819.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print(f\"Loading X2 image from: {X2_image_path}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtarget_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mX2_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1649/3786562627.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(image_path, target_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         transform = transforms.Compose([\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         transforms.Resize(target_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1649/3488204961.py\u001b[0m in \u001b[0;36mpreprocess_input_img\u001b[0;34m(test_image_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# Resizing image to desired input size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   gray_resized_test_img = cv2.resize(gray_sample_test_img, targetsize,\n\u001b[0;32m---> 23\u001b[0;31m                       interpolation = cv2.INTER_AREA)   # To shrink an image\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# Remove blemishes from image (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(nn.ParameterList(model.parameters()), lr=learning_rate)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# train_maes = []\n",
    "# val_maes = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "#    running_mae = 0.0\n",
    "#    running_corrects = 0\n",
    "    for i, (inputs, poses) in enumerate(train_loader):\n",
    "            (target_images, X2_images), poses = inputs, poses\n",
    "            target_images, X2_images, poses = target_images.to(device), X2_images.to(device), poses.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            p1_x, p2_x,  p3_x = model(target_images, X2_images)\n",
    "            loss = criterion(p1_x,  p2_x,  p3_x, poses)\n",
    "#             optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "#         running_mae += mae.item()\n",
    "#         running_corrects += accuracy(outputs, labels)\n",
    "\n",
    "#             if i % 20 == 0:\n",
    "#                 print(\"iteration: \" + str(epoch) + \"\\n    \" + \"Loss is: \" + str(loss)\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "#     train_maes.append(running_mae / len(train_loader))\n",
    "#     train_accuracies.append(running_corrects / len(train_loader))\n",
    "        \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "#     val_mae = 0.0\n",
    "#     val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            (target_images, X2_images), poses = data\n",
    "            target_images, X2_images, poses = target_images.to(device), X2_images.to(device), poses.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            p1_x, p1_q, p2_x = model(target_images, X2_images)\n",
    "            loss = criterion(p1_x, p1_q, p2_x, poses)\n",
    "#             mae = mae_criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "#             val_mae += mae.item()\n",
    "#             val_corrects += accuracy(outputs, labels)\n",
    "    \n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "#     val_maes.append(val_mae / len(val_loader))\n",
    "#     val_accuracies.append(val_corrects / len(val_loader))\n",
    "    \n",
    "#     save_results(epoch, train_losses[-1], val_losses[-1], train_maes[-1], val_maes[-1], train_accuracies[-1], val_accuracies[-1], output_dir)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "print('Training Done')\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'results_{current_time}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eY3rPw7wPVEe",
    "outputId": "213b5a1c-d9f4-4659-f6dc-b6ff9d0d1a44"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Initialize lists to store losses, MAE, and accuracy\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# train_maes = []\n",
    "# val_maes = []\n",
    "# train_accuracies = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # Define MAE criterion\n",
    "# mae_criterion = nn.L1Loss()\n",
    "\n",
    "# # Define accuracy function\n",
    "# def accuracy(outputs, labels):\n",
    "#     preds = outputs.round()\n",
    "#     corrects = torch.sum(torch.all(preds == labels, dim=1)).item()\n",
    "#     return corrects / len(labels)\n",
    "\n",
    "# # Training loop\n",
    "# EPOCHS = 70\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     running_mae = 0.0\n",
    "#     running_corrects = 0\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "#         (target_images, X2_images), labels = inputs, labels\n",
    "#         target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(target_images, X2_images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         mae = mae_criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "#         running_mae += mae.item()\n",
    "#         running_corrects += accuracy(outputs, labels)\n",
    "# #         print(\"=\")\n",
    "        \n",
    "#     # Store the training loss, MAE, and accuracy\n",
    "#     train_losses.append(running_loss / len(train_loader))\n",
    "#     train_maes.append(running_mae / len(train_loader))\n",
    "#     train_accuracies.append(running_corrects / len(train_loader))\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_mae = 0.0\n",
    "#     val_corrects = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data in val_loader:\n",
    "#             (target_images, X2_images), labels = data\n",
    "#             target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "#             outputs = model(target_images, X2_images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             mae = mae_criterion(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "#             val_mae += mae.item()\n",
    "#             val_corrects += accuracy(outputs, labels)\n",
    "    \n",
    "#     val_losses.append(val_loss / len(val_loader))\n",
    "#     val_maes.append(val_mae / len(val_loader))\n",
    "#     val_accuracies.append(val_corrects / len(val_loader))\n",
    "    \n",
    "#     save_results(epoch, train_losses[-1], val_losses[-1], train_maes[-1], val_maes[-1], train_accuracies[-1], val_accuracies[-1], output_dir)\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {train_losses[-1]:.4f}, Validation Loss: {val_losses[-1]:.4f}, Train MAE: {train_maes[-1]:.4f}, Validation MAE: {val_maes[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Validation Accuracy: {val_accuracies[-1]:.4f}')\n",
    "\n",
    "# print('Training Done')\n",
    "\n",
    "# # Plot the training and validation loss, MAE, and accuracy\n",
    "# plt.figure(figsize=(18, 6))\n",
    "\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(val_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Loss')\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.plot(train_maes, label='Train MAE')\n",
    "# plt.plot(val_maes, label='Validation MAE')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation MAE')\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.plot(train_accuracies, label='Train Accuracy')\n",
    "# plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.savefig(os.path.join(output_dir, 'tinplace=Trueraining_validation_plots.png'))\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeVRGN56hkNZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# total_loss = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in val_loader:\n",
    "#         (target_images, X2_images), labels = data\n",
    "#         target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "# #         print(target_images.size())\n",
    "        \n",
    "#         outputs = model(target_images, X2_images)\n",
    "# #         print(outputs.size())\n",
    "# #         print(\"____\\n\")\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         total_loss += loss.item()\n",
    "# #         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# avg_loss = total_loss / len(val_loader)\n",
    "# print('Validation Loss: ', avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Define a function to convert continuous values to discrete classes\n",
    "# def continuous_to_discrete(y):\n",
    "#     y_discrete = np.zeros_like(y)\n",
    "#     y_discrete[y > 0.5] = 1\n",
    "#     y_discrete[y < -0.5] = -1\n",
    "#     return y_discrete\n",
    "\n",
    "# # Define a function to extract true and predicted values\n",
    "# def extract_true_and_predicted_values(loader, model, device):\n",
    "#     model.eval()\n",
    "#     true_y1 = []\n",
    "#     true_y2 = []\n",
    "#     pred_y1 = []\n",
    "#     pred_y2 = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data in loader:\n",
    "#             (target_images, X2_images), labels = data\n",
    "#             target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "#             outputs = model(target_images, X2_images)\n",
    "\n",
    "#             true_y1.extend(labels[:, 0].cpu().numpy())\n",
    "#             true_y2.extend(labels[:, 1].cpu().numpy())\n",
    "#             pred_y1.extend(outputs[:, 0].cpu().numpy())\n",
    "#             pred_y2.extend(outputs[:, 1].cpu().numpy())\n",
    "\n",
    "#     return np.array(true_y1), np.array(pred_y1), np.array(true_y2), np.array(pred_y2)\n",
    "\n",
    "# # Extract true and predicted values\n",
    "# true_y1, pred_y1, true_y2, pred_y2 = extract_true_and_predicted_values(val_loader, model, device)\n",
    "\n",
    "# # Convert continuous predictions and true values to discrete classes\n",
    "# true_y1_discrete = continuous_to_discrete(true_y1)\n",
    "# pred_y1_discrete = continuous_to_discrete(pred_y1)\n",
    "# true_y2_discrete = continuous_to_discrete(true_y2)\n",
    "# pred_y2_discrete = continuous_to_discrete(pred_y2)\n",
    "\n",
    "# # print(true_y1_discrete-pred_y1_discrete)\n",
    "# # print()\n",
    "# # Generate confusion matrices\n",
    "# cm_y1 = confusion_matrix(true_y1_discrete, pred_y1_discrete , labels=[-1, 0, 1])\n",
    "# cm_y2 = confusion_matrix(true_y2_discrete, pred_y2_discrete , labels=[-1, 0, 1])\n",
    "\n",
    "# # Plot confusion matrices\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# # ConfusionMatrixDisplay(cm_y1).plot(ax=ax[0])\n",
    "# sns.heatmap(cm_y1, annot=True, fmt='d', cmap='Blues', ax=ax[0], cbar=False)\n",
    "\n",
    "# ax[0].set_title('Confusion Matrix for y1')\n",
    "# ax[0].set_xticklabels(['-1', '0', '1'])\n",
    "# ax[0].set_yticklabels(['-1', '0', '1'])\n",
    "\n",
    "# # ConfusionMatrixDisplay(cm_y2).plot(ax=ax[1])\n",
    "# sns.heatmap(cm_y2, annot=True, fmt='d', cmap='Blues', ax=ax[1], cbar=False)\n",
    "\n",
    "# ax[1].set_title('Confusion Matrix for y2')\n",
    "# ax[1].set_xticklabels(['-1', '0', '1'])\n",
    "# ax[1].set_yticklabels(['-1', '0', '1'])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image_path = './processed/D9/img026.jpg'\n",
    "# target_image_path= './processed/D9/target.jpg'\n",
    "\n",
    "# # Load the images\n",
    "# target_image = load_image(target_image_path).unsqueeze(0).to(device)  # Adding batch dimension\n",
    "# test_image = load_image(test_image_path).unsqueeze(0).to(device)     # Adding batch dimension\n",
    "\n",
    "# print(\"target state image is of shape\",target_image.shape)  # Expected: torch.Size([1, 1, 200, 200])\n",
    "# print(\"current state frame is of shape\",test_image.shape)    # Expected: torch.Size([1, 1, 200, 200])\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(target_image, test_image)\n",
    "#     print(\"model predictions\",outputs)\n",
    "#     nparray = outputs.cpu().numpy()  # Move tensor to CPU before conversion\n",
    "# #     if nparray[0]<0.5 and nparray[0]>-0.5:\n",
    "# #         nparray[0]=0\n",
    "# #     if nparray[0]>0.5:\n",
    "# #         nparray[0]=1\n",
    "# #     if nparray[0]<-0.5:\n",
    "# #         nparray[0]=-1\n",
    "    \n",
    "# #     print(\"one hot encoding\",nparray.size)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obf7QfWYOBFT"
   },
   "outputs": [],
   "source": [
    "# net = ResNet50(10).to('cuda')\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image_path = './processed/D5/img021.jpg'\n",
    "# target_image_path= './processed/D5/target.jpg'\n",
    "\n",
    "# # Load and preprocess the images\n",
    "# target_image = load_image(target_image_path).to(device)\n",
    "# test_image = load_image(test_image_path).to(device)\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# # model.eval()\n",
    "\n",
    "# # Disable gradient calculation\n",
    "# with torch.no_grad():\n",
    "#     # Get the model's output\n",
    "#     output = model(target_image, test_image)\n",
    "\n",
    "# # Print the output\n",
    "# print('Model Output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Sp3I3vApPK2w",
    "outputId": "60d8be96-76cf-4ca8-8451-9ef862d1ef19"
   },
   "outputs": [],
   "source": [
    "# EPOCHS = 30\n",
    "# for epoch in range(EPOCHS):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "#         (target_images, X2_images), labels = inputs, labels\n",
    "#         target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(target_images, X2_images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         if i % 10 == 0 and i > 0:\n",
    "#             print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss / 100)\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Training Done')\n",
    "\n",
    "# # Validation loop\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in val_loader:\n",
    "#         (target_images, X2_images), labels = data\n",
    "#         target_images, X2_images, labels = target_images.to(device), X2_images.to(device), labels.to(device)\n",
    "#         outputs = model(target_images, X2_images)\n",
    "\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy on validation dataset: ', 100 * (correct / total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_image_path= './processed/D4/img001.jpg'\n",
    "\n",
    "\n",
    "# def preprocess_input_img(test_image_path):\n",
    "#     '''\n",
    "#     This function takes the path to the input test image\n",
    "#     and returns a preprocessed image (which can be used as a input to the model)\n",
    "#     (Input): Single test image path\n",
    "#     (Output): Preprocessed image\n",
    "#     '''\n",
    "\n",
    "#     # Read the original test image\n",
    "#     orig_sample_test_img = cv2.cvtColor(cv2.imread(test_image_path), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     x = 50\n",
    "#     y = 0\n",
    "#     width = 200\n",
    "#     height = 200\n",
    "\n",
    "# # Crop the image\n",
    "#     orig_sample_test_img = orig_sample_test_img[y:y+height, x:x+width]\n",
    "\n",
    "\n",
    "#     # Convert image to gray scale\n",
    "#     gray_sample_test_img = cv2.cvtColor(orig_sample_test_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "#     # Resizing image to desired input size\n",
    "#     gray_resized_test_img = cv2.resize(gray_sample_test_img, (285, 285),\n",
    "#                         interpolation = cv2.INTER_AREA)   # To shrink an image\n",
    "\n",
    "#     # Remove blemishes from image (if any)\n",
    "#     (thresh, black_n_white_sample_img) = cv2.threshold(gray_resized_test_img, 70,255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "#     # Display Images\t: Plot Sample Input and Preprocessed Test Image\n",
    "\n",
    "#     f = plt.figure(figsize=(10,5))\n",
    "#     ax1 = f.add_subplot(121)\n",
    "#     ax2 = f.add_subplot(122)\n",
    "\n",
    "#     ax1.imshow(np.squeeze(orig_sample_test_img), cmap='gray')\n",
    "#     ax1.set_title(\"Original Test Input Image\", pad=15, fontsize=13, fontweight='bold')\n",
    "#     ax2.imshow(np.squeeze(black_n_white_sample_img), cmap='gray')\n",
    "#     ax2.set_title(\"Preprocessed Test Input Image\", pad=15, fontsize=13, fontweight='bold')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     return orig_sample_test_img, black_n_white_sample_img\n",
    "\n",
    "\n",
    "# x,y = preprocess_input_img(target_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CIFAR10-ResNet50_85%.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
